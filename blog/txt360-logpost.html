<!DOCTYPE HTML>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0FFN6N7318"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-0FFN6N7318');
    </script>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

    <!-- SEO -->
    <title>TxT360: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend | LLM360</title>
    <meta name="description" content="We introduce TxT360 (Trillion eXtracted Text), the first dataset to globally deduplicate 99 CommonCrawl snapshots and 14 high-quality data sources from diverse domains (e.g., FreeLaw, PG-19, etc.)."/>
    <link rel="canonical" href="https://www.llm360.ai/blog/txt360-logpost.html" />
    <meta name="keywords" content="TxT360, Open Source AI, Artificial General Intelligence, LLM360, AI Research, Amber-7B, CrystalCoder-7B, LLM Training"/>
    <meta name="author" content="LLM360"/>
    <meta name="robots" content="index, follow"/>

    <!-- Open Graph Protocol -->
    <meta property="og:title" content="TxT360: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend | LLM360"/>
    <meta property="og:description" content="We introduce TxT360 (Trillion eXtracted Text), the first dataset to globally deduplicate 99 CommonCrawl snapshots and 14 high-quality data sources from diverse domains (e.g., FreeLaw, PG-19, etc.)."/>
    <meta property="og:type" content="website"/>
    <meta text property="og:url" content="https://www.llm360.ai/blog/txt360-logpost.html"/>
    <meta property="og:image" content="https://www.llm360.ai/images/open-graph-image.jpg"/>

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@llm360">
    <meta name="twitter:title" content="TxT360: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend | LLM360">
    <meta name="twitter:description" content="We introduce TxT360 (Trillion eXtracted Text), the first dataset to globally deduplicate 99 CommonCrawl snapshots and 14 high-quality data sources from diverse domains (e.g., FreeLaw, PG-19, etc.).">
    <meta name="twitter:image" content="https://www.llm360.ai/images/open-graph-image.jpg">

    <!-- Schema Markup -->
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "TxT360: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend | LLM360",
        "description": "LLM360 is excited to announce the release of TxT360 (Trillion eXtracted Text), the first dataset to globally deduplicate 99 CommonCrawl snapshots and 14 high-quality data sources from diverse domains (e.g., FreeLaw, PG-19, etc.).",
        "image": "https://www.llm360.ai/images/open-graph-image.jpg",
        "keywords": "TxT360, Open Source AI, Artificial General Intelligence, LLM360, AI Research, Amber-7B, CrystalCoder-7B, LLM Training",
        "articleSection": "Artificial Intelligence",
        "datePublished": "2024-10-07",
        "url": "https://www.llm360.ai/blog/txt360-logpost.html",
        "publisher": {
            "@type": "Organization",
            "name": "LLM360",
            "logo": {
                "@type": "ImageObject",
                "url": "https://www.llm360.ai/images/logo-highres.png"
            },
            "sameAs": [
                "https://twitter.com/llm360",
                "https://github.com/LLM360",
                "https://arxiv.org/abs/2312.06550"
            ]
        },
        "articleBody": "LLM360 is excited to announce several new releases to further our mission enabling community-owned AGI by creating standards and tools to advance the bleeding edge of LLM capability and empower knowledge transfer, research, and development. K2-65B, the first fully reproducible large language model to outperform Llama 2 70B, using 35% less compute, has demonstrated on par reasoning and text generation capabilities with strong domain knowledge in medicine, coding, and math..."
    }
    </script>

    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="../assets/favicon/favicon.ico" />
    <link rel="icon" type="image/png" sizes="192x192" href="../assets/favicon/android-chrome-192x192.png">
    <link rel="icon" type="image/png" sizes="512x512" href="../assets/favicon/android-chrome-512x512.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../assets/favicon/favicon-16x16.png">
    <link rel="apple-touch-icon" sizes="180x180" href="../assets/favicon/apple-touch-icon.png">
    <!-- <link rel="manifest" href="/site.webmanifest"> -->
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="../assets/css/main.css" />
    <noscript><link rel="stylesheet" href="../assets/css/noscript.css" /></noscript>
	
  <style>
    .bibtex-block {
      background: #f5f5f5;
      border-left: 4px solid #888;
      padding: 1em;
      font-family: monospace;
      overflow-x: auto;
    }
  </style>
	
</head>
<body class="is-preload">

<!-- Sidebar -->
<button class="toggle-btn" id="toggleBtn">☰</button>
<section id="sidebar" class="sidebar">
    <div class="inner">
        <nav>
            <a class="alt" href="../index.html">
                <figure class="hover-rotate">
                    <img src="../images/logo-highres.png" alt="logo" />
                </figure>
            </a>
            <h2>LLM360</h2>
            <ul>
                <li><a href="../index.html#datasets">Datasets</a></li>
                <li><a href="../index.html#models">Models</a></li>
                <li><a href="index.html#projects">Projects</a></li>
                <li><a href="../index.html#paper">Papers</a></li>
                <li><a href="../index.html#blogs">Blogs</a></li>
<!--                <li><a href="../community.html">Open-source Communities</a></li>-->
                <li><a href="../about.html">About</a></li>
<!--                <li><a href="../joinus.html">Join Us</a></li>-->
<!--                <li><a href="../index.html#six">Get in touch</a></li>-->
            </ul>
        </nav>
    </div>
</section>

<!-- Wrapper -->
<div id="wrapper">

    <!-- Intro -->
    <section id="seven" class="wrapper fullscreen fade-up">
        <div class="inner">
            <h1><strong>TxT360</strong>: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend</h1>
            <p>LLM360 Team  &nbsp | &nbsp Oct 07, 2024</p>
            <h4>
                Read the full blogpost <a href="https://huggingface.co/spaces/LLM360/TxT360" target="_blank">here</a>
            </h4>
	    <br>
<div class="bibtex-block">
@misc{txt360data2024,
      title={TxT360: A Top-Quality LLM Pre-training Dataset Requires the Perfect Blend}, 
      author={Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu, Eric P. Xing},
      year={2024}
}
</div>	
            <br>
            <div class="content">
                <h2>About TxT360</h2>
                <span class="image right">
						<img src="../images/txt360_logo.png" alt="" />
                </span>
                <p>
                    TL;DR We introduce TxT360 (Trillion eXtracted Text), the first dataset to globally deduplicate 99 CommonCrawl snapshots and 14 high-quality data sources from diverse domains (e.g., FreeLaw, PG-19, etc.). The large-scale deduplication process and rich metadata stored enables precise control over data distribution. We demonstrate a simple but effective upsampling recipe that creates a 15+ trillion-token corpus, outperforming FineWeb 15T on several key metrics. With the information, TxT360 empowers pre-trainers to explore more advanced weighting techniques, a feature not commonly available in previous pre-training datasets. Our findings highlight the importance of both high-quality data sources and appropriate weighting for optimal blending in LLM training.
                    In line with our 360° open source spirit, we document all detailed steps, reasons of our decisions, detailed statistics, our code (stay tuned!), analysis results and more, in additional to the dataset itself. We hope this can serve as a useful resource for future developers.
                </p>
                <span class="image fit">
						<img src="../images/MMLU_Performance.png" alt="" />
                </span>
                <p>
                    Building on top of the prior studies on pre-training data, TxT360 carefully implements data processing steps including extraction, filtering, deduplication, personally identifiable information removal, and other steps. Unlike DCLM and RedPajama V2, we also hope to provide a dataset at this scale that is ready to go, without requiring futher filtering.
                </p>
                <h3>How to Read this Blog Post?</h3>
                <p>
                    This document contains all the details and is lengthy. We recommend readers to use the Table of Contents to jump to the appropriate sections. The post might also be slightly too long for mobile reading (sorry!). At each top level section, we provided a quick guide for the content. We also recommend readers to consider this post as a reference for some high level statistics related to pre-training datasets.
                    </br>
                    Advanced blog navigation elements are available on laptops and larger viewing windows.
                </p>
                <h2>Why TxT360</h2>
                <p>
                    In this year we have seen excellent datasets released by the community. Among those, most datasets focus on one source (e.g., crawled websites, code bases, papers). However, it is not trivial to combine these sources together due to the potential duplicaiton across them. TxT360 is the first dataset to combine most of sources commonly used in pretraining.
<!--                    <span class="image fit">-->
<!--						<img src="../images/txt_table1.png" alt="" />-->
<!--                    </span>-->
                <table>
                    <thead>
                        <tr>
                            <th>Data Source</th>
                            <th>TxT360</th>
                            <th>FineWeb</th>
                            <th>RefinedWeb</th>
                            <th>PedPajamaV2</th>
                            <th>C4</th>
                            <th>Dolma</th>
                            <th>RedPajamaV1</th>
                            <th>The Pile</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>CommonCrawl Snapshots</td>
                        <td><strong>99</strong></td>
                        <td>96</td>
                        <td>90</td>
                        <td>84</td>
                        <td>1</td>
                        <td>24</td>
                        <td>5</td>
                        <td>0.6% of 74</td>
                    </tr>
                    <tr>
                        <td>Papers</td>
                        <td><strong>5 Sources</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>1 Source</td>
                        <td>1 Source</td>
                        <td>4 Sources</td>
                    </tr>
                    <tr>
                        <td>Wikipedia</td>
                        <td><strong>310+ Languages</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>Included</td>
                        <td>English Only</td>
                    </tr>
                    <tr>
                        <td>FreeLaw</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>-</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>DM Math</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>-</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>USPTO</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>-</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>PG-19</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>Included</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>HackerNews</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>Ubuntu IRC</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>EuroParl</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>StackExchange</td>
                        <td><strong>Included</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>Included</td>
                    </tr>
                    <tr>
                        <td>Code</td>
                        <td><strong>**</strong></td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>-</td>
                        <td>Included</td>
                        <td>Included</td>
                        <td>Included</td>
                    </tr>
                </table>
                    In LLM pretraining, it is common to combine all possible text sources due to the Scaling Law. Crawled web pages are included to provide a vast quantity of data which can cover long tail and diverse information, while curated datasets such as Wikipedia are also used, which often provide the 'deep-dive' domain information. By integrating the reach of web data with the quality of curated sources, TxT360 meets and surpasses the rigorous standards required for state-of-the-art LLM pre-training.
                    </br>
                    ** TxT360 does not include very specific domains such as code and math. This decision was made due to the perceived low duplication code with other sources, and the different logic requiring to build those datasets. We leave those work to future work and recommend users refer to existing projects such as Stack V2.
                </p>
                <h2>Our Approach</h2>
                <p>
                    To produce TxT360, a comprehensive data processing pipeline was designed to account for the nuances of both web and curated datasets. The pipeline presents a unified framework for processing both data types, making it convenient and easily adaptive for users to revise and fine-tune the pipeline for their own use cases.
                    Web datasets are inherently noisy and varied. The TxT360 pipeline implements sophisticated filtering and deduplication techniques to clean and remove redundancies while preserving data integrity.
                    Curated datasets are typically structured and consistently formatted, but also can cause troubles with their own special formatting preferences. TxT360 filters these sources with selective steps to maintain their integrity while providing seamless integration into the larger dataset. Both data source types are globally deduplicated together resulting in ~5T tokens of high-quality data. The table below shows the source distribution of TxT360 tokens. <strong>Note that we do not recommend to use the raw distribution of the deduplicated dataset, a simple recipe is provided in the studies section.</strong>
                <table>
                    <thead>
                        <tr>
                            <th>Data Source</th>
                            <th>Raw Data Size</th>
                            <th>Token Count</th>
                            <th>Information Cut-Off Date</th>
                        </tr>
                    </thead>
                    <tr>
                        <td>CommonCrawl</td>
                        <td>9.2 TB</td>
                        <td>4.83T</td>
                        <td>2024-30</td>
                    </tr>
                    <tr>
                        <td>Papers</td>
                        <td>712 GB</td>
                        <td>154.96B</td>
                        <td>Q4 2023</td>
                    </tr>
                    <tr>
                        <td>Wikipedia</td>
                        <td>199 GB</td>
                        <td>35.97B</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>FreeLaw</td>
                        <td>71 GB</td>
                        <td>16.7B</td>
                        <td>Q1 2024</td>
                    </tr>
                    <tr>
                        <td>DM Math</td>
                        <td>22 GB</td>
                        <td>5.23B</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>USPTO</td>
                        <td>45 GB</td>
                        <td>4.95B</td>
                        <td>Q3 2024</td>
                    </tr>
                    <tr>
                        <td>PG-19</td>
                        <td>11 GB</td>
                        <td>2.63B</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>HackerNews</td>
                        <td>4.1 GB</td>
                        <td>1.08B</td>
                        <td>Q4 2023</td>
                    </tr>
                    <tr>
                        <td>Ubuntu IRC</td>
                        <td>4.7 GB</td>
                        <td>1.54B</td>
                        <td>Q3 2024</td>
                    </tr>
                    <tr>
                        <td>EuroParl</td>
                        <td>6.1 GB</td>
                        <td>1.96B</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>StackExchange</td>
                        <td>79 GB</td>
                        <td>27B</td>
                        <td>Q4 2023</td>
                    </tr>
                </table>
                    We provide details and context for the choices behind TxT360 in the respective Common Crawl Data Processing and Curated Source Processing section. A deep dive describing the deduplication process can be found in the Shared Processing Steps section.
                </p>
            </div>
        </div>
    </section>
</div>

<!-- Footer -->
<footer id="footer" class="wrapper style1-alt">
    <div class="inner">
        <ul class="menu">
            <p>
                LLM360, proudly sponsored by Petuum and MBZUAI, is dedicated to advancing the field of AI by providing comprehensive access to large language models.<br>
                LLM360 enables community-owned AGI by creating standards and tools to advance the bleeding edge of LLM capability and empower knowledge transfer, research, and development.
            </p>
            <ul class="actions">
                <li><a href="https://twitter.com/llm360" target="_blank" class="icon brands circle fa-twitter"><span class="label">Twitter</span></a></li>
                <li><a href="https://discord.gg/jFPq9AycBu" target="_blank" class="icon brands circle fa-discord"><span class="label">Discord</span></a></li>
                <li><a href="https://github.com/LLM360" target="_blank" class="icon brands circle fa-github"><span class="label">Github</span></a></li>
                <li><a href="mailto:team@llm360.ai" target="_blank" class="icon circle fa-envelope"><span class="label">Email</span></a></li>
            </ul>
            <p class="copyright">&copy; LLM360 2023-2024. All rights reserved.</p>
        </ul>
    </div>
</footer>

<!-- Scripts -->
<script src="../assets/js/jquery.min.js"></script>
<script src="../assets/js/jquery.scrollex.min.js"></script>
<script src="../assets/js/jquery.scrolly.min.js"></script>
<script src="../assets/js/browser.min.js"></script>
<script src="../assets/js/breakpoints.min.js"></script>
<script src="../assets/js/util.js"></script>
<script src="../assets/js/main.js"></script>
<script>
    document.addEventListener('DOMContentLoaded', function () {
        const sidebar = document.getElementById('sidebar');
        const toggleBtn = document.getElementById('toggleBtn');
        const content = document.getElementById('content');

        toggleBtn.addEventListener('click', function () {
            sidebar.classList.toggle('hidden');
            content.classList.toggle('expanded');
        });

        function checkScreenSize() {
            if (window.innerWidth < 1280) {
                sidebar.classList.add('hidden');
            } else {
                sidebar.classList.remove('hidden');
            }
        }

        window.addEventListener('resize', checkScreenSize);
        checkScreenSize(); // Initial check
    });

    // Create the button element
    const backToTopButton = document.createElement('button');
    backToTopButton.id = 'back-to-top';
    backToTopButton.textContent = 'Top';
    document.body.appendChild(backToTopButton);

    // Show or hide the button based on scroll position
    window.onscroll = function() {
        if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
            backToTopButton.style.display = 'block';
        } else {
            backToTopButton.style.display = 'none';
        }
    };

    // Scroll to the top when the button is clicked
    backToTopButton.onclick = function() {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
    };
</script>

</body>
</html>
