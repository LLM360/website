<!DOCTYPE HTML>
<html lang="en">
	<head>
		<!-- Google tag (gtag.js) -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-0FFN6N7318"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-0FFN6N7318');
		</script>

		<title>Introducing LLM360: Fully Transparent open-source LLMs | LLM360</title>
		<meta name="description" content="We are thrilled to introduce LLM360, an initiative to open source LLMs that fosters transparency, trust, and collaborative research. When releasing models under LLM360, we strive to make all the details of LLM accessible to everyone." />
		<link rel="canonical" href="https://www.llm360.ai/blog/introducing-llm360-fully-transparent-open-source-llms.html" />
		<meta name="author" content="Petuum, Mohamed bin Zayed University of Artificial Intelligence"/>
		<meta name="robots" content="index, follow"/>

		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

		<!-- Favicon -->
		<link rel="icon" type="image/x-icon" href="../static-assets/favicon/favicon.ico" />
		<link rel="icon" type="image/png" sizes="192x192" href="../static-assets/favicon/android-chrome-192x192.png">
		<link rel="icon" type="image/png" sizes="512x512" href="../static-assets/favicon/android-chrome-512x512.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../static-assets/favicon/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../static-assets/favicon/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="../static-assets/favicon/apple-touch-icon.png">
		<!-- <link rel="manifest" href="/site.webmanifest"> -->
		<meta name="msapplication-TileColor" content="#da532c">
		<meta name="theme-color" content="#ffffff">

		<!-- Open Graph -->
		<meta property="og:title" content="Introducing LLM360: Fully Transparent Open-Source LLMs" />
		<meta property="og:type" content="article" />
		<meta property="og:url" content="https://www.llm360.ai/blog/introducing-llm360-fully-transparent-open-source-llms.html" />
		<meta property="og:image" content="https://www.llm360.ai/images/open-graph-image.jpg" />
		<meta property="og:description" content="We are thrilled to introduce LLM360, an initiative to open source LLMs that fosters transparency, trust, and collaborative research. When releasing models under LLM360, we strive to make all the details of LLM accessible to everyone." />
		<meta property="og:site_name" content="LLM360" />

		<!-- Twitter Card -->
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:title" content="Introducing LLM360: Fully Transparent Open-Source LLMs">
		<meta name="twitter:description" content="We are thrilled to introduce LLM360, an initiative to open source LLMs that fosters transparency, trust, and collaborative research. When releasing models under LLM360, we strive to make all the details of LLM accessible to everyone.">
		<meta name="twitter:url" content="https://www.llm360.ai/blog/introducing-llm360-fully-transparent-open-source-llms.html">
		<meta name="twitter:image" content="https://www.llm360.ai/images/open-graph-image.jpg">
		<meta name="twitter:site" content="@llm360">
		<meta name="twitter:creator" content="@llm360">

		<!-- Schema Markup -->
		<script type="application/ld+json">
			{
				"@context": "http://schema.org",
				"@type": "ScholarlyArticle",
				"headline": "Introducing LLM360: Fully Transparent Open-Source LLMs",
				"description": "Introducing LLM360 - an initiative for fully transparent open-source large language models...",
				"image": "https://www.llm360.ai/images/open-graph-image.jpg",
				"keywords": "LLM, Open Source, AI, Machine Learning, LLM360",
				"articleSection": "Artificial Intelligence",
				"author": [
					{"@type": "Person", "name": " Liu, Zhengzhong"},
					{"@type": "Person", "name": " Qiao, Aurick"},
					{"@type": "Person", "name": " Neiswanger, Willie"},
					{"@type": "Person", "name": " Wang, Hongyi"},
					{"@type": "Person", "name": " Tan, Bowen"},
					{"@type": "Person", "name": " Tao, Tianhua"},
					{"@type": "Person", "name": " Li, Junbo"},
					{"@type": "Person", "name": " Pangarkar, Omkar"},
					{"@type": "Person", "name": " Fan, Richard"},
					{"@type": "Person", "name": " Gu, Yi"},
					{"@type": "Person", "name": " Miller, Victor"},
					{"@type": "Person", "name": " Zhuang, Yonghao"},
					{"@type": "Person", "name": " He, Guowei"},
					{"@type": "Person", "name": " Li, Haonan"},
					{"@type": "Person", "name": " Ranjan, Nikhil"},
					{"@type": "Person", "name": " Shen, Zhiqiang"},
					{"@type": "Person", "name": " Ren, Xuguang"},
					{"@type": "Person", "name": " Iriondo, Roberto"},
					{"@type": "Person", "name": " Mu, Cun"},
					{"@type": "Person", "name": " Hu, Zhiting"},
					{"@type": "Person", "name": " Schulze, Mark"},
					{"@type": "Person", "name": " Nakov, Preslav"},
					{"@type": "Person", "name": " Baldwin, Tim"},
					{"@type": "Person", "name": " Xing, Eric"}
				],
				"datePublished": "2023-12-11",
				"url": "https://www.llm360.ai/blog/introducing-llm360-fully-transparent-open-source-llms.html",
				"publisher": {
					"@type": "Organization",
					"name": "LLM360",
					"logo": {
						"@type": "ImageObject",
						"url": "https://www.llm360.ai/images/logo-highres.png"
					},
					"sameAs": [
            "https://twitter.com/llm360",
            "https://github.com/LLM360",
            "https://arxiv.org/abs/2312.06550"
        	]
				},
				"articleBody": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers.However, most LLMs have only released partial artifacts, such as the final modelweights or inference code, and technical reports increasingly limit their scope tohigh-level design choices and surface statistics. These choices hinder progress inthe field by degrading transparency into the training of LLMs and forcing teams torediscover many details in the training process. We present LLM360, an initiativeto fully open-source LLMs, which advocates for all training code and data, modelcheckpoints, and intermediate results to be made available to the community. Thegoal of LLM360 is to support open and collaborative AI research by making theend-to-end LLM training process transparent and reproducible by everyone. As afirst step of LLM360, we release two 7B parameter LLMs pre-trained from scratch,AMBER and CRYSTALCODER, including their training code, data, intermediatecheckpoints, and analyses (at llm360.ai). We are committed to continuallypushing the boundaries of LLMs through this open-source effort. More large-scaleand stronger models are underway and will be released in the future."
			}
		</script>

		<link rel="stylesheet" href="../static-assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../static-assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

	<!-- Sidebar -->
	<button class="toggle-btn" id="toggleBtn">☰</button>
	<section id="sidebar" class="sidebar">
		<div class="inner">
			<nav>
				<a class="alt" href="../index.html">
					<figure class="hover-rotate">
						<img src="../images/logo-highres.png" alt="logo" />
					</figure>
				</a>
				<h2>LLM360</h2>
				<ul>
                        <li><a href="blog"><span style="color: #FF6347; font-weight: bold;">(New!)</span> ✨ Blog</a></li>
                        <li><a href="#datasets">Datasets</a></li>
                        <li><a href="#models">Models</a></li>
                        <li><a href="#projects">Projects</a></li>
                        <li><a href="#paper">Papers</a></li>
                        <li><a href="#news">News</a></li>
                        <li><a href="about.html">About</a></li>
				</ul>
			</nav>
		</div>
	</section>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Intro -->
		<section id="seven" class="wrapper fullscreen fade-up">
			<div class="inner">
				<h1>Introducing <strong>LLM360</strong>: Fully Transparent Open-Source LLMs</h1>
				<p>LLM360 Team  &nbsp | &nbsp December 11, 2023</p>
				<div class="content">
					<h2>Introduction</h2>
					<span class="image right">
						<img src="../images/blog-banner.png" alt="" />
					</span>
					<p>
						In recent months, the open-source large language model
						(LLM) community has seen tremendous model contributions.
						However, model weight releases and overview technical
						reports do not contain enough information to cover the
						complexity of LLM training, which hinders openness and
						transparency, the mechanisms behind trustworthy and
						innovative research and science for decades.
						</br>
						To this end, we are thrilled to introduce LLM360, an
						initiative to open source LLMs that fosters
						transparency, trust, and collaborative research.  When
						releasing models under LLM360, we strive to make all the
						details of LLM accessible to everyone.
						</br>
						Most open-source LLM releases include model weights and
						evaluation results.
						However, additional information is often needed to
						genuinely understand a model's behavior—and this
						information is not typically available to most
						researchers.
						Hence, we commit to releasing all of the intermediate
						checkpoints (up to 360!) collected during training, all
						of the training data (and its mapping to checkpoints),
						all collected metrics (e.g., loss, gradient norm,
						evaluation results), and all source code for
						preprocessing data and model training.
						These additional artifacts can help researchers and
						practitioners to have a deeper look into LLM’s
						construction process and conduct research such as
						analyzing model dynamics.
						We hope that LLM360 can help make advanced LLMs more
						transparent, foster research in smaller-scale labs, and
						improve reproducibility in AI research.
						</br>
						To start, we are releasing two models under LLM360:
						Amber-7B and Crystal-7B, which we hope epitomize
						the spirit of open-source and transparent AI
						development.
						</br>
						LLM360 is proudly sponsored by Petuum, MBZUAI, and Cerebras.
					</p>
					<hr class="major" />
					<h2>Achieving Transparency and Collaborative Research through LLM360</h2>
					<p>
						The basis of LLM360 is to create a framework that encourages openness and
						research collaboration for large language models. Currently, we include
						all of the following artifacts associated for models in the LLM360 family:
					</p>
					<ul>
						<li><strong>Frequent Intermediate Model Checkpoints:</strong> During training, model parameters and optimizer states are collected regularly. These artifacts can offer valuable insights for studying LLM training dynamics and how it scales with data, and allows one to resume training at various stages.</li>
						<li><strong>Training Data with Full Data Sequence:</strong> The entire preprocessed, tokenized training dataset is fully disclosed and made publicly available. The dataset is presented exactly in correspondence to the training steps.</li>
						<li><strong>Source Code:</strong> All the code used, including data processing, training, evaluation, and analysis.</li>
						<li><strong>Logs and Metrics:</strong> All the training logs,evaluations and analysis results collected during training are publicly disclosed, also in correspondence to the training steps and data sequence.​</li>
					</ul>
					<p>
						This is just a beginning to our open source efforts and we are committed to continue providing more details.
						Please don’t hesitate to let us know what you want to know! We are thrilled to receive community feedback to continually refine and augment our releases.
					</p>
					<hr class="major" />
					<h2>Amber and Crystal Released under LLM360</h2>
					<p>
						<span class="image left">
							<img src="../images/ac_underl360.png">
						</span>
						The first two models to be released under LLM360 are Amber and Crystal.
						Amber is a 7B English LLM and Crystal is a 7B code & text LLM.
						</br>
						Both models are released under the Apache 2.0 license.
					</p>
					</br>
					<h3>Amber: Advancing Knowledge and Transparency in LLM Pretraining</h3>
					<p>
						Amber is the inaugural member of the LLM360 family, accompanied by its fine-tuned versions: AmberChat and AmberSafe.
						Amber adopts a model architecture consistent with LLaMA 7B, and we adhere closely to LLaMA's hyperparameters.
						We notice that Amber performs well in MMLU but slightly worse on ARC.
					</p>
					<div class="row aln-center">
						<span class="image fit">
							<img src="../images/table_amber.png"alt="" />
						</span>
						<p><strong>Figure 1: Open LLM leaderboard comparisons among a few notable LLMs</strong></p>
					</div>
					<p>Amber's true superpower lies in facilitating a knowledge exchange between the training team and the wider community.
						Along with the customary final model weights, Amber is released with 359 additional model checkpoints (360 total) and the per-step data sequence for each checkpoint.
						</br>
						Providing access to these intermediate checkpoints can be beneficial to both researchers looking to advance the capability and understanding of LLMs and industry teams who are pretraining or customizing LLMs for enterprise purposes.
						</br>
						We will release  more specific learnings and insights from training Amber,
						stay tuned for further posts! At this moment, we recommend you check out the metrics and analysis below.
					</p>
					<br>
					<ul class="actions fit">
						<li><a href="https://short.llm360.ai/amber-metrics" target="_blank" class="button">Wandb</a></li>
						<li><a href="https://short.llm360.ai/amber-data" target="_blank" class="button">Data</a></li>
						<li><a href="https://short.llm360.ai/amber-code" target="_blank" class="button">Code</a></li>
						<li><a href="https://short.llm360.ai/amber-model" target="_blank" class="button">Model</a></li>
					</ul>
					<div class="row aln-center">
						<span class="image small">
							<img src="../images/amber_logo.png" alt=""/>
						</span>
					</div>
					</br>
					<h3>Crystal: Bridging Human Language and Machine Code</h3>
					<p>Crystal is a 7B language model trained on 1.4 trillion tokens, achieving a balance between coding and language ability.</p>
					<div class="row aln-center">
						<span class="image fit">
							<img src="../images/table_cc.png" alt=""/>
						</span>
						<p><strong>
							Figure 2: Evaluation comparisons among a few notable code and language models.
							The last column is the average of the language task average and the code task average.
							Crystal strikes a good balance between both language and code tasks.
						</strong></p>
					</div>
					<p>Unlike most previous code LLMs, Crystal is trained using a careful mixture of text and code data to maximize utility in both domains.
						Code data is introduced earlier during the pretraining process (as compared with Code Llama 2 which is fine-tuned on Llama 2 using entirely code data).
						Additionally, we trained Crystal on Python and web programming language, to improve its utility as a programming assistant.
						</br>
						Our experiments show that Crystal achieves a balanced position between LLaMA 2 and Code LLaMA,
						but with fewer training tokens (LLaMA2 7B is trained on 2T tokens and Code LLaMA is trained with additional 600B tokens).
						The graph below plots the language and coding ability of each model based on the tables above.
						As evidenced by the evaluations, LLaMA 2 regresses in language ability when fine-tuned on code.
						More research is needed to fully understand the phenomena, but studying Crystal may offer some insights.
					</p>
					<div class="row aln-center">
						<span class="image fit ">
							<img src="../images/good_balance.png"  alt=""/>
						</span>
						<p><Strong>Figure 3: CRYSTAL shows a good balance
							of language and coding abilities. The y-
							axis is the average over ARC-C, HellaSwag,
							MMLU, and GSM8K. The x-axis is the aver-
							age of MBPP and HumanEval.</Strong></p>
					</div>
					<p>
						By excelling at both language and code, Crystal proves useful for investigating AI Agent and tool use capabilities.
						Crystal is released with 143 checkpoints and all pre training data.
						</br>
						The model was trained on the Condor Galaxy 1 supercomputer built by Cerebras and G42.
					</p>
					<ul class="actions fit">
						<li><a href="https://short.llm360.ai/crystalcoder-metrics" target="_blank" class="button">Wandb</a></li>
						<li><a href="https://short.llm360.ai/crystalcoder-data" target="_blank" class="button">Data</a></li>
						<li><a href="https://short.llm360.ai/crystalcoder-code" target="_blank" class="button">Code</a></li>
						<li><a href="https://short.llm360.ai/crystalcoder-model" target="_blank" class="button">Model</a></li>
					</ul>
					<div class="row aln-center">
						<span class="image fit small"><img src="../images/crystalcoder_logo.png" alt=""/></span>
					</div>
					<hr class="major" />
					<h2>Goals of the LLM360 Framework</h2>
					<ul>
						<li>
							<h4>Increased Accessibility:</h4>
							<ul>
								<li>0 GPUs: the community can view all important intermediate results as if training just finished.</li>
								<li>1+ GPUs: intermediate checkpoints can be trained without needing to start from scratch, opening up wider research opportunities.</li>
							</ul>
						</li>
						<li>
							<h4>Research Advancement, Reproducibility, and Model Understanding:</h4>
							<ul>
								<li>We hope this project lays the groundwork for future research by offering complete, reproducible resources.</li>
								<li>By replicating studies and verifying results, we foster a reliable and transparent research environment.</li>
							</ul>
						</li>
						<li>
							<h4>Environmental Responsibility:</h4>
							<ul>
								<li>LLM360 promotes sustainable research by sharing all intermediate results that can be extended upon, thereby reducing unnecessary compute.</li>
							</ul>
						</li>
					</ul>
					<hr class="major" />
					<h2>Collaboration and Community in LLM360</h2>
					<h3>Contributing to the LLM360 Ecosystem</h3>
					<p>LLM360 thrives on community involvement, offering various ways for researchers, developers, and enthusiasts to engage and contribute. Here’s a streamlined guide to getting involved:
					</p>
					<ul>
						<li>
							<h4>Get Involved:</h4>
							<ul>
								<li><strong>GitHub:</strong> Our <a href="https://github.com/llm360" target="_blank" rel="noopener">GitHub page</a> is the hub for all code related to LLM360. Explore, modify, or use our code and contribute your improvements.</li>
								<li><strong>HuggingFace:</strong> Access and download LLM360 models on <a href="https://huggingface.co/llm360" target="_blank" rel="noopener">HuggingFace</a>. Experiment with them, and share your findings or applications.</li>
							</ul>
						</li>
						<li>
							<h4>Share Your Work:</h4>
							<ul>
								<li><strong>Research Contributions:</strong> If you’ve used Amber or Crystal for research, we encourage you to share your results. Your insights can help enhance these models.</li>
								<li><strong>Share Results:</strong> Your analysis results on any of the checkpoints are more than welcome. Feel free to share with us metrics you compute, we will host selected metrics on our public Weights & Biases dashboard.</li>
							</ul>
						</li>
						<li>
							<h4>Feedback and Suggestions:</h4>
							<ul>
								<li><strong>Feedback Form:</strong> We value your input. <a href="../index.html#contact">Use this form</a> to provide feedback or suggest improvements. Let us know what you want to know more about LLMs!</li>
								<li><strong>Join Discussions:</strong> Engage with peers on our forums. Share experiences, ask questions, and exchange ideas.</li>
							</ul>
						</li>
						<li>
							<h4>Collaborate</h4>
							<ul>
								<li><strong>Partnership Opportunities: </strong>If you're interested in collaborating on a project or have an idea, we’d love to hear from you.</li>
							</ul>
						</li>
					</ul>
					<hr class="major" />
					<h2>What's Ahead for LLM360</h2>
					<p>
						LLM360 is on a mission to expand and deepen the influence of AI research by providing fully accessible, open-source LLMs. We are committed to being fully open and sharing more high quality information on LLMs.
						</br>
						Join our global community of researchers, developers, and AI enthusiasts to explore, enhance, and expand models under LLM360.  Together, we can make AI research more open, transparent, and trustworthy.
					</p>
					<hr class="major" />
					<h2>Resources</h2>
					<ul>
						<li><a href="https://huggingface.co/LLM360" target="_blank" rel="noopener">Download the models</a></li>
						<li><a href="https://github.com/llm360" target="_blank" rel="noopener">Access our codebase</a></li>
						<li><a href="../LLM360-Towards-Fully-Transparent-Open-Source-LLMs.pdf" target="_blank">Read the paper</a></li>
					</ul>
					<hr class="major" />
					<h2><label for="citation">Citation</label></h2>
					<p> If you use LLM360, feel free to cite: </p>
					<div class="box" background-color="#eee" border="1px" solid="#999" display="block" padding="20px">
						<textarea style="margin-bottom:1em" id="citation" rows="9" cols="50" readonly>@article{liu2023llm360,
	title={LLM360: Towards Fully Transparent Open-Source LLMs},
	author={Liu, Zhengzhong and Qiao, Aurick and Neiswanger, Willie and Wang, Hongyi and Tan, Bowen and Tao, Tianhua and Li, Junbo and Wang, Yuqi and Sun, Suqi and Pangarkar, Omkar and Fan, Richard and Gu, Yi and Miller, Victor and Zhuang, Yonghao and He, Guowei and Li, Haonan and Koto, Fajri and Tang, Liping and Ranjan, Nikhil and Shen, Zhiqiang and Ren, Xuguang and Iriondo, Roberto and Mu, Cun and Hu, Zhiting and Schulze, Mark and Nakov, Preslav and Baldwin, Tim and Xing, Eric},
	year={2023}}
						</textarea>
						<button onclick="copyCitation()">Copy Citation</button>
						<script>
							function copyCitation() {
								var copyText = document.getElementById("citation");
								copyText.select();
								document.execCommand("copy");
							}
						</script>
					</div>
				</div>
			</div>
		</section>
	</div>

	<!-- Footer -->
	<footer id="footer" class="wrapper style1-alt">
		<div class="inner">
			<ul class="menu">
				<p>
					LLM360, proudly sponsored by Petuum and MBZUAI, is dedicated to advancing the field of AI by providing comprehensive access to large language models.<br>
					LLM360 enables community-owned AGI by creating standards and tools to advance the bleeding edge of LLM capability and empower knowledge transfer, research, and development.
				</p>
				<ul class="actions">
					<li><a href="https://twitter.com/llm360" target="_blank" class="icon brands circle fa-twitter"><span class="label">Twitter</span></a></li>
					<li><a href="https://discord.gg/jFPq9AycBu" target="_blank" class="icon brands circle fa-discord"><span class="label">Discord</span></a></li>
					<li><a href="https://github.com/LLM360" target="_blank" class="icon brands circle fa-github"><span class="label">Github</span></a></li>
					<li><a href="mailto:team@llm360.ai" target="_blank" class="icon circle fa-envelope"><span class="label">Email</span></a></li>
				</ul>
				<p class="copyright">&copy; LLM360 2023-2024. All rights reserved.</p>
			</ul>
		</div>
	</footer>

	<!-- Scripts -->
	<script src="../static-assets/js/jquery.min.js"></script>
	<script src="../static-assets/js/jquery.scrollex.min.js"></script>
	<script src="../static-assets/js/jquery.scrolly.min.js"></script>
	<script src="../static-assets/js/browser.min.js"></script>
	<script src="../static-assets/js/breakpoints.min.js"></script>
	<script src="../static-assets/js/util.js"></script>
	<script src="../static-assets/js/main.js?v=2025072102"></script>
	<script>
		document.addEventListener('DOMContentLoaded', function () {
			const sidebar = document.getElementById('sidebar');
			const toggleBtn = document.getElementById('toggleBtn');
			const content = document.getElementById('content');

			toggleBtn.addEventListener('click', function () {
				sidebar.classList.toggle('hidden');
				content.classList.toggle('expanded');
			});

			function checkScreenSize() {
				if (window.innerWidth < 1280) {
					sidebar.classList.add('hidden');
				} else {
					sidebar.classList.remove('hidden');
				}
			}

			window.addEventListener('resize', checkScreenSize);
			checkScreenSize(); // Initial check
		});

		// Create the button element
		const backToTopButton = document.createElement('button');
		backToTopButton.id = 'back-to-top';
		backToTopButton.textContent = 'Top';
		document.body.appendChild(backToTopButton);

		// Show or hide the button based on scroll position
		window.onscroll = function() {
			if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
				backToTopButton.style.display = 'block';
			} else {
				backToTopButton.style.display = 'none';
			}
		};

		// Scroll to the top when the button is clicked
		backToTopButton.onclick = function() {
			document.body.scrollTop = 0;
			document.documentElement.scrollTop = 0;
		};
	</script>

	</body>
</html>
